[TOC]

# AdaBoost

AdaBoost:Adaptive Boosting，自适应提升算法。是分类算法中的集成算法。

集成算法有两种模式：投票选举(bagging)、再学习(boosting)

投票选举模式：做决定时，让K个模型分别进行分类，然后选择出现次数最多的类作为最终分类结果。

再学习模式：把K个分类器（弱分类器）进行加权融合，形成一个新的分类器(强分类器)

## **AdaBoost工作原理**

AdaBoost算法通过训练多个弱分类器，将它们组成一个强分类器。强分类器是由一系列的弱分类器根据不同的权重组合而成。

假设弱分类器为$G_i(x)$，它在强分类器中的权重$\alpha_i$,得出强分类器f(x):

​			 	$f(x)=\sum\limits_{i=1}^n \alpha_i G_i(x)​$

若弱分类器分类效果好，权重增大，如果弱分类器分类效果一般，权重应该降低，权重决定于弱分类器对样本的分类错误率。

 			$ \alpha_i = \dfrac{1}{2}\log^{\dfrac{1-e_i}{e_i}}$

通过改变样本的数据分布选择最优的弱分类器。AdaBoost 会判断每次训练的样本是否正确分类，对于正确正确分类的样本，降低它的权重，对于被错误分类的样本，增加它的权重。再基于上一次得到的分类准确率，来确定这次训练样本中每个样本的权重。然后将修改过权重的新数据集传递给下一层的分类器进行训练。

可以用 $D_{k+1}$ 代表第 k+1 轮训练中，样本的权重集合，其中 $W_{k+1},1$ 代表第 k+1 轮中第一个样本的权重,以此类推 $W_{k+1},N$代表第k+1轮中第N个样本的权重；

​		$D_{k+1} = (w_{k+1,1},W_{k+1,2}…,W_{k+1,N})$

第k+1轮中的样本权重，是根据该样本在第K轮的权重以及第k个分类器的准确率而定，

​		$W_{k+1,i} = \dfrac{W_k,i}{Z_k} \times e^{-\alpha _k y_i G_k(x_i)},i = 1,2,…,N$

AdaBoost算法是一个框架，可以指定任意的分类器，通常可以采用CART分类器作为弱分类器。

