[TOC]

#**朴素贝叶斯**

朴素贝叶斯它是一种简单但极为强大的预测建模算法

##**贝叶斯原理**

贝叶斯概率针对"逆向概率"提出，它建立在主观判断的基础上，在不了解客观事实的情况下，可以先估计一个值，然后根据实际结果不断进行修正。

贝叶斯原理概念：

先验概率：通过经验来判断事情发生的概率

后验概率：发生结果之后，推测原因的概率

条件概率：事件 A 在另外一个事件 B 已经发生条件下的发生概率表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”

似然函数：

似然函数就是用来衡量模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。

贝叶斯公式为：

​			$P(B_i|A) = \dfrac{P(B_i)P(A|B_i)}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)}$

通用贝叶斯公式为：

​                         $P(B_i| A) = \dfrac{P(B_i)P(A|B_i)}{\sum\limits_{i=1}^nP(B_i)P(A|B_i)}$

朴素贝叶斯的概率：

1.每个类别的概率$P(C_j)$ 

2.每个属性的条件概率$P(Ai|Cj)$

## **贝叶斯原理、贝叶斯分类、朴素贝叶斯 区别**

贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。

##**朴素贝叶斯分类工作原理**

朴素贝叶斯分类工作原理分为两种：

1.离散数据

2.连续数据

针对于离散数据可以直接根据通用公式求得分类。

对于连续数据，要先得到正态分布的密度函数。正态分布密度函数的获得通过几个参数：正态分布中需要计算的数值、正态分布平均值、正态分布标准差、逻辑值：true为累计分布，false为概率密度。

##**朴素贝叶斯分类器工作流程**

朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。

朴素贝叶斯分类器需要三个流程：

一：准备阶段  需要人工完成

二：训练阶段  输入是特征属性和训练样本，输出孙分类器

三： 应用阶段 使用分类器对心数据进行分类

## **sklearn朴素贝叶斯分类**

高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布

多项式朴素贝叶斯：：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。

伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，符合 0/1 分布，在文档分类中特征是单词是否出现。



伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。

##**TF-IDF值**

TF-IDF 是一个统计方法，用来评估某个词语对于一个一个文件集或文档库中的其中一份文件的重要程度。TF-IDF 实际上是两个词组Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了逆向文档频率。

词频 TF计算了一个单词在文档中出现的次数，逆向文档频率 IDF，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。。IDF 越大就代表该单词的区分度越大。所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。 公式为：

​			$词频TF= \dfrac{单词出现的次数}{该文档的总单词数}$

​                        $逆向文档频率IDF= log\dfrac{文档总数}{该单词出现的文档数 + 1}$

​                       $TF-IDF = TF\times IDF$

在sklearn中可以直接使用TfidfVectorizer类计算单词TF-IDF 向量的值

概念：停用词 stop_words ，一个列表List类型

过滤规则 token_pattern ，正则表达式



```python
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> tfidf_vec = TfidfVectorizer()
>>> documents = ['this is the bayes document', 'this is the second second document','and the third one','is this the document']
>>> tfidf_matrix = tfidf_vec.fit_transform(documents)
>>> #输出文档中所有不重复的词
>>> print ('不重复的词:', tfidf_vec.get_feature_names())
不重复的词: ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']
>>> #输出每个单词的对应的id值
>>> print('每个单词的ID:', tfidf_vec.vocabulary_)
每个单词的ID: {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}
>>> #输出每个单词在每个文档中的TF-IDF值
>>> print('每个单词的tfidf值', tfidf_matrix.toarray())
每个单词的tfidf值 [[0.         0.63314609 0.40412895 0.40412895 0.         0.
  0.33040189 0.         0.40412895]
 [0.         0.         0.27230147 0.27230147 0.         0.85322574
  0.22262429 0.         0.27230147]
 [0.55280532 0.         0.         0.         0.55280532 0.
  0.28847675 0.55280532 0.        ]
 [0.         0.         0.52210862 0.52210862 0.         0.
  0.42685801 0.         0.52210862]]
>>> 
```



##**对文档进行分类**

对文档进行分类，有两个重要阶段

1.基于分词的数据准备，包括分词、单词权重计算、去掉停用词；

2.应用朴素贝叶斯分类进行分类，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。

模块介绍：

1.对文档进行分词

​	英文文档中，最常用的是 NTLK 包。

```python
import nltk
word_list = nltk.word_tokenize(text) # 分词
nltk.pos_tag(word_list) # 标注单词的词性
```

​        中文文档中，常用的jieba包

```python
import jieba
word_list = jieba.cut (text) # 中文分词
```

2.加载停用词表

读取文件，获取停用词表

```python
stop_words = [line.strip() for line in io.open('/Users/apple/Desktop/GitHubProject/Read mark/数据分析/geekTime/data/stopwords.txt', encoding = 'utf-8').readlines()]
```

3.计算单词的权重

```python 
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
features = tf.fit_transform(train_contents)
```

这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50%的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。

4.生成朴素贝叶斯分类器

当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。当 0 < alpha < 1,使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，，迭代次数越多，精度越高。

```python
# 多项式贝叶斯分类器
from sklearn.naive_bayes import MultinomialNB  
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
```

5.使用生成的分类器做预测

```python
test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)
test_features=test_tf.fit_transform(test_contents)
predicted_labels=clf.predict(test_features)
```

6.计算准确率

```python
from sklearn import metrics
print metrics.accuracy_score(test_labels, predicted_labels)
```

##**中文文档分类实践**

```python
>>> train_contents=[]
>>> train_labels=[]
>>> test_contents=[]
>>> test_labels=[]
>>> #  导入文件
>>> import os
>>> import io
>>> path = '/Users/apple/Desktop/GitHubProject/Read mark/数据分析/geekTime/data/textclassification/'
>>> start=os.listdir(path +'train')
>>> for item in start:
	test_path=path +'test/'+item+'/'
	train_path=path + '/train/'+item+'/'
	for file in os.listdir(test_path):
		with open(test_path+file,encoding="GBK") as f:
			 test_contents.append(f.readline())
			 test_labels.append(item)
	for file in os.listdir(train_path):
		with open(train_path+file,encoding='gb18030', errors='ignore') as f:
			train_contents.append(f.readline())
			train_labels.append(item)

			
>>> print (len(test_labels),len(test_contents))
200 200
>>> import jieba
>>> from sklearn import metrics
>>> from sklearn.naive_bayes import MultinomialNB
>>> stop_words = [line.strip() for line in io.open(path+'stop/stopword.txt',encoding = 'utf-8').readlines()]
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> tf = TfidfVectorizer(tokenizer=jieba.cut, stop_words=stop_words, max_df = 0.5)
>>> train_features = tf.fit_transform(train_contents)
>>> clf = MultinomialNB(alpha=0.001).fit(train_features,train_labels)
>>> test_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words,max_df=0.5,vocabulary=tf.vocabulary_)
>>> test_features=test_tf.fit_transform(test_contents)
>>> predicted_labels = clf.predict(test_features)
>>> print (metrics.accuracy_score(test_labels,predicted_labels))
0.925
>>> 
```

