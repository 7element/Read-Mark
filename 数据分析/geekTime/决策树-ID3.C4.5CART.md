[TOC]

#决策树

决策树一个树状逻辑结构图，有根节点、子节点、叶子结点

## **决策树工作原理**

做决策树的时候会经历两个阶段：**构造和剪枝**

##**构造**

构造就是生成一颗完整的决策树。**构造的过程就是选择什么属性成为节点的过程**。

构造过程存在三种节点：

1.根节点：就是树的最顶端，最开始的那个节点。

2.内部节点：就是树中间的那些节点

3.叶节点：就是树最底部的节点

构造过程中需要解决的三个重要问题：

1.选择那种属性作为根节点

2.选择哪些属性作为子节点

3.什么时候停止并得到目标状态，即叶子结点

##**剪枝**

剪枝就是给决策树瘦身，这一步不需要太多的判断，同样可以得到不错的结果。要防止**"过拟合"(Overfitting)**现象的发生。

**过拟合**是指模型的训练结果太过完美，以至于在实际应用的过程中，会存在"死板"的结果，导致分类错误。预支对应的是欠拟合。

 过拟合结果造成的重要原因之一：训练集中样本数量较小。

数量小选择的属性过多，会把训练集中一些数据的特点当成所有数据的特点，但该特点不一定是全部数据的特点。造成决策树在真是的数据分类中出现错误，也就是模型的**泛化能力**差。

**泛化能力**：分类器通过训练集抽象出来的分类能力。

剪枝分为`预剪枝`(Pre-Pruning ) 和`后剪枝`(Post-Pruning)

预剪枝是在决策树构造时就进行剪枝，方法是在构造过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分意义不大，这时就会把当前节点作为叶子结点，不对其进行划分。

 后剪枝就是在生成决策树之后再进行剪枝，从决策树的叶子结点开始，逐层向上对每个节点进行评估。如果剪掉该节点子树与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就把该节点子树进行剪枝。方法是用这个节点子树的叶子结点来代替该节点，类标记为这个节点子树中最频繁的那个类。

在以下训练集基础上构建决策树

![](images/决策树训练集.png)



##**纯度和信息熵**

纯度：让目标变量的分歧最小，可以把决策树的构造过程理解成为寻找纯净划分的过程。

信息熵(entropy)：表示信息的不确定度。数学公式 

![](images/信息熵计算公式.png)

例如两个集合：

集合 1：5 次去打篮球，1 次不去打篮球；

集合 2：3 次去打篮球，3 次不去打篮球。

用公式求得

集合1的信息熵E(t)= 0.65

集合2的信息熵E(t)= 1

从计算结果中可以得出结论：信息熵越大，纯度越低。当集合中的所有样本均匀混合，信息熵最大，纯度最低。

构造决策树会基于纯度来构建，经典的"不纯度"指标有三种：**信息增益(ID3算法)**、**信息增益率(C4.5算法)** 以及**基尼指数(Cart算法)**

##**ID3算法**

ID3算法计算的是信息增益，信息增益指的是划分可以带来纯度的提高，信息熵的下降。计算公式是：父亲节点的信息熵减去所有子节点的信息熵。

![](images/信息增益公式.png)

首先计算根节点的信息熵，共有7条数据，3次打篮球，4次不打篮球

Ent(D) = -( $ \dfrac{3}{7} ​$  $\log _2 ^ {\tfrac{3}{7}} ​$ + $\dfrac{4}{7}​$ $\log_2^{\tfrac{4}{7}}​$ ) = 0.985

**.**如果以天气作为属性划分，会有$D_1$、$D_2$ 、$D_3$

$D_1​$ (天气=晴天) = {1次打，2次不打}

$D_2$ (天气=阴天) = {1次打，1次不打}

$D_3​$ (天气=小雨) = {1次打，1次不打}

Ent($D_1​$) = -($\dfrac{1}{3}​$$\log_2^{\tfrac{1}{3}}​$ + $\dfrac{2}{3}\log_2^{\tfrac{2}{3}}​$) = 0.918

Ent($D_2$) =1

Ent($D_3​$) = 1

∴归一化信息熵 = $\dfrac{3}{7} \times 0.918 + \dfrac{2}{7} \times1 +\dfrac{2}{7} \times 1​$ = 0.965

∴G(D,天气) = 0.985 - 0.965 = 0.020

**.**如果温度作为属性划分，会有$D_1$、$D_2$ 、$D_3$

$D_1$ (温度=高) = {2次打，2次不打}

$D_2$ (温度=低) = {1次不打}

$D_3$ (温度=中) = {1次打，1次不打}

Ent($D_1$) = 1

Ent($D_2$) = -$\log_2^1$ = 0

Ent($D_3$) = 1

∴归一化信息熵 = $\dfrac{4}{7} + 0 + \dfrac{2}{7}\times1$ =0.857

∴G(D,温度) = 0.985 - 0.857 = 0.128

同样可得到

G(D,湿度) = 0.020

G(D,刮风) = 0.020

综上可得，温度的信息增益最高，所以选择温度作为根节点。

然后分别针对温度的$D_1 D_2 D_3$ 三个子集往下划分算其不同属性）(天气、湿度、刮风)作为节点的信息增益，最终得到以下决策树。

![](images/打篮球决策树.jpg)

ID3优点：方法简单

ID3 缺点：

（1）有些属性对分类任务没有太大作用，但是仍然可能会作为最优属性。

（2）容易产生过拟合。

（3） 训练数据如果有少量错误，可能会产生决策树分类错误。



##**C4.5算法**

因为ID3存在着缺陷，在ID3算法上改进形成来C4.5算法

1.采用信息增益率

 ID3在计算的时候，倾向于选择去至多的属性。为了避免这个问题，C4.5采用信息增益率方式来选择属性。信息增益率 = 信息增益/属性熵 

当属性有很多值得时候，相当于被划分成来许多份，虽然信息增益变大来，但是对于C4.5来说，属性熵也会变大，所以整体的信息增益率并不大。

2.采用悲观剪枝

ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。

悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

3.离散化处理连续属性

C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。

4.处理缺失值

针对数据集不完整的情况，C4.5 也可以进行处理。若样本在这个属性上有缺失值，则对样本进行划分的时候不考虑缺失值。

C4.5算法缺点：构造决策树，需要对数据进行多次扫描和排序，效率低

##**CART算法**

CART算法：Classification And Regression Tree ，分类回归树。ID3 和C4.5算法可以生成二叉树或多叉树，而CART算法只支持二叉树。同时CART算法比较特殊，既可以作为分类树，又可以作为回归树。

分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续的数值进行预测，也就是数据在某个区间内都有去值的可能，它输出的是一个数值。

CART分类树的工作流程