[TOC]

#决策树构建

决策树一个树状逻辑结构图，有根节点、子节点、叶子结点

## **决策树工作原理**

做决策树的时候会经历两个阶段：**构造和剪枝**

##**构造**

构造就是生成一颗完整的决策树。**构造的过程就是选择什么属性成为节点的过程**。

构造过程存在三种节点：

1.根节点：就是树的最顶端，最开始的那个节点。

2.内部节点：就是树中间的那些节点

3.叶节点：就是树最底部的节点

构造过程中需要解决的三个重要问题：

1.选择那种属性作为根节点

2.选择哪些属性作为子节点

3.什么时候停止并得到目标状态，即叶子结点

##**剪枝**

剪枝就是给决策树瘦身，这一步不需要太多的判断，同样可以得到不错的结果。要防止**"过拟合"(Overfitting)**现象的发生。

**过拟合**是指模型的训练结果太过完美，以至于在实际应用的过程中，会存在"死板"的结果，导致分类错误。预支对应的是欠拟合。

 过拟合结果造成的重要原因之一：训练集中样本数量较小。

数量小选择的属性过多，会把训练集中一些数据的特点当成所有数据的特点，但该特点不一定是全部数据的特点。造成决策树在真是的数据分类中出现错误，也就是模型的**泛化能力**差。

**泛化能力**：分类器通过训练集抽象出来的分类能力。

剪枝分为`预剪枝`(Pre-Pruning ) 和`后剪枝`(Post-Pruning)

预剪枝是在决策树构造时就进行剪枝，方法是在构造过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分意义不大，这时就会把当前节点作为叶子结点，不对其进行划分。

 后剪枝就是在生成决策树之后再进行剪枝，从决策树的叶子结点开始，逐层向上对每个节点进行评估。如果剪掉该节点子树与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就把该节点子树进行剪枝。方法是用这个节点子树的叶子结点来代替该节点，类标记为这个节点子树中最频繁的那个类。

在以下训练集基础上构建决策树

![](images/决策树训练集.png)



##**纯度和信息熵**

纯度：让目标变量的分歧最小，可以把决策树的构造过程理解成为寻找纯净划分的过程。

信息熵(entropy)：表示信息的不确定度。数学公式 

![](images/信息熵计算公式.png)

例如两个集合：

集合 1：5 次去打篮球，1 次不去打篮球；

集合 2：3 次去打篮球，3 次不去打篮球。

用公式求得

集合1的信息熵E(t)= 0.65

集合2的信息熵E(t)= 1

从计算结果中可以得出结论：信息熵越大，纯度越低。当集合中的所有样本均匀混合，信息熵最大，纯度最低。

