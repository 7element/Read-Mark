[TOC]

#**KNN**

KNN：K-Nearest Neighbor

##**KNN工作原理**

KNN原理分为三步：

1.计算待分类物体与其他物体之间的距离；

2.统计距离最近的 K 个邻居；

3.对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属该类

## **K值选择**

K值选择较小，会产生过拟合，K值选择较大，会产生欠拟合。实践中，K值的选取采用交叉验证的方式。

## **距离计算方式**

距离计算有五种，前三种比较常见

1.欧氏距离-欧几里得距离

​        	计算公式：

​				$d =\sqrt[]{ (x_1-x_2)^2 + (y_1-y_2)^2}​$

2.曼哈顿距离

​		计算公式：

​				$d= |x_1-x_2| + [y_1-y_2]​$

3.闵可夫斯基距离

​		计算公式:

​				$d = \sqrt[p]{\sum\limits_{i=1}^n|x_i-y_i|^p}$

​           当p=1 ，就是曼哈顿距离，当p=2，就是欧氏距离，当p$\rightarrow\infty$,	   就是切比雪夫距离。

4.切比雪夫距离

​	两个点坐标数值差的绝对值的最大值，用数学表示就是：					$max(|x_1-x_2|,|y_1-y_2|)$

5.余弦距离

​           余弦计算两个向量的夹角，对绝对值不感兴趣。在兴趣相关性比较上，角度关系比距离的绝对值更重要，蔚县距离可以用于衡量用户对内容兴趣的区分度。

##**KD树**

KD树：K-Dimensional ，是一种每个节点都是k维数值点的二叉树。可以减少计算距离次数，提升KNN的搜索效率。

##**用KNN做回归**

对于一个新点，找出k个最近邻居，将他们的属性加权评均赋给该点。

