[TOC]

#**数据采集：如何自动化采集**

数据的走势，由多个维度影响，需要多源的数据采集，收集尽可能多的数据维度，保证数据质量，得到高质量的数据挖掘结果。多数时候自动切换IP以及云采集是自动化采集的关键。

##**数据源从采集角度方面划分为四类**：

​	开放数据源：政府、企业、高校

​	爬虫抓取：网页、App

​	日志采集：前端采集、后端脚本

​	传感器：图像、测速、热敏

##**如何使用开放数据源**

使用开放数据源可以从多个维度考虑：

1.单位维度：政府、高校

2.行业维度：交通、金融、能源

​	单位维度的数据源：

​	|欧盟|提供欧盟各机构的大量数据|http://open-data.europa.eu/en/data/|

​        |Facebook|查询该网站用户公开的海量信息|https://developers.facebook,com/docs/graph-api	|	

​	|Amazo|亚马逊网络服务开放数据集|http://aws.amazon.com/datasets|

​	|Google|谷歌金融，40年以来的股票数据，实时更新|http://www.google.com/finance|

​	|北京大学|北京大学开放研究数据平台|http://opendata.pku.edn.cn|

​	|ImageNet|图像识别量最大的数据库，包括近1500万张图像|http://www.image-net.org|

##**如何使用爬虫抓取**

1.Python爬虫  Requests、XPath、Pandas是Python的三个利器。其它还有Selenium、PhantommJs

2.第三方抓取贡酒：

​	火车采集器：抓取、数据清洗、数据分析、数据挖掘、可视化，适用于大多数网页

​	八爪鱼：免费采集模块是内容采集规则，包括电商、生活服务类、社交媒体类和论坛类

​                       云采集是配置好采集任务，通过云端多节点并发采集，还可以自动切换多个IP

​	集搜客：完全可视化，无需编程。无云采集功能，

##**如何使用日志采集工具**

日志采集最大的作用，就是通过分析用户访问情况，提升系统的性能，从而提高系统承载量。及时发现系统承载瓶颈，可以方便技术人员基于用户实际的访问情况进行优化。

日志采集形式：

1.通过Web服务器采集，例如httpd、Nginx、Tomcat等自带日志记录功能。同时很多互联网企业有自己的海量数据采集工具，多用于系统日志采集，如Hadoop的Chukwa、Clouder的Flume、Facebook的Scribe

2.自定义采集用户行为，例如用JavaScript代码监听用户的行为，AJAX异步请求后台记录日志等

##**埋点**

埋点就是在有需要的位置采集相应的信息，进行上报。包括用户信息、设备信息；用户停留时长等

在需要统计数据的地方植入统计代码，植入代码可以自己写，也可以使用第三方统计工具。需要自己写的diam，都是主营核心业务，对于埋点监测性工具，许多成熟的第三方工具，比如友盟、Google Analysis、Talkingdata等。



--------------------------------------------

#**数据采集：用八爪鱼采集微博上的"D&G"评论**

采集流程步骤：

输入网页->输入关键词->点击搜索

流程设计：设置翻页 提取数据

启动采集： 启动本地采集

在设置流程的时候一定要先翻页再提取元素。遇到问题要用好流程视图和XPath

# **Python爬虫：自动化下载海报**

爬虫流程：打开网页、提取数据和保存数据

打开网页：使用Requests访问页面，得到服务器返回的HTML页面以及JSON数据

提取数据：针对HTML页面，使用XPath定位，提取数据；针对JSON数据，使用JSON进行解析

保存数据：使用Pandas保存数据，最后导出CSV文件

## **Requests 访问请求页面**

Requests 是Python HTTP的客户端库，编写爬虫的时候都会用到。Requests有两种方式访问：Get和Post。

用Get请求代码访问如下:

```python
>>> import requests as req
>>> r = req.get('http://www.douban.com')
```

用Post请求代码如下：

```python
r= req.post('http://xxx.com',data={'key':'value'})
```



## **XPath定位**

XPath是XML的路径语言，实际上是通过元素和属性进行导航，来定位位置。

​	node:选取node节点的所有子节点

​	/：从根节点选取

​	//:选取所有的当前节点，不考虑他们的位置

​	.当前节点

​	..父节点

​	@属性选择

​	| 或，两个节点的合计

​	text()当前路径下的文本内容

for example：

xpath('node')选取node节点的所有子节点

xpath('/div') 从根节点上选取div节点

xpath('//div')选取所有的div节点

xpath('./div')选取当前节点下的div节点

xpath('//@id')选取所有的id属性

XPath提供了100个内建函数，来做匹配，功能非常强大。使用XPath定位，会用到Python的解析库lxml。

若要定位到HTML中的所有列表项目，可以采用如下代码

```python
>>> from lxml import etree
>>> html = 'http.www.douban.com'
>>> html = etree.HTML(html)
>>> result = html.xpath('//li')
```

## **JSON对象**

JSON是一种轻量级的交互方式，在Python中有JSON库可以用来转换

```python
json.dumps() #将Python对象转换成json对象
json.loads() #将JSON对象转换成Python对象
```

for example：

```python
>> import json
>>> jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}'
>>> input = json.loads(jsonData)
>>> print (input)
{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}
```

## 如何使用JSON数据自动下载

知道了Python爬虫的基本原理和实现，事件一下使用JSON数据自动下载

如果爬取的页面是动态页面，需要关注XHR数据。XHR用于后台与服务器交换数据。利用Chrome浏览器的开发者工具可以在插件中查看XHR数据。

在豆瓣搜索中对王祖贤进行搜索，发现XHR中一个请求如下：

https://www.douban.com/j/search_photo?q=王祖贤&limit=20&start=0///

```python
#下载图片函数
>>> picpath = '/Users/apple/Downloads/' + '王祖贤'
>>> def download(src, id):
	dir = picpath +'/' + str(id) + '.jpg'
	try:
		pic = req.get(src, timeout = 10)
		fp = open(dir, 'wb')
		fp.write(pic.content)
		fp.close()
	except req.exceptions.ConnectionError:
		print ('图片无法下载')
```

用JSON方式下载的函数代码

```python
>>> if not os.path.isdir(picpath):
	os.mkdir(picpath)
 >>> query = '王祖贤'
>>> for i in range(0, 22471, 20):
	url = 'https://www.douban.com/j/search_photo?q='+query+'&limit=20&start='+str(i)
	html = req.get(url).text
	response = json.loads(html,encoding='utf-8')
	for image in response['images']:
		download(image['src'],image['id'])
```

## 使用XPath方式自动下载

利用JSON请求方式，会受到加载的限制，只有JS都加载完毕后才能获取完整的HTML。但XPath可以不受加载的限制，帮我们定位想要的元素。

利用XPath进行下载需要用XPath定位图片的网址，以及电影的名称。快速定位XPPath的方法是采用浏览器的XPath Helper插件，快捷键是Ctrl + Shift + X,按住Shift 用鼠标选中想要的定位元素，会显示选中元素的XPath。

但有时候网页未加载完会导致直接用Request获取HTML的时候，发现想要的XPath不存在。因此需要一个工具进行网页模拟，这个工具是Python中的Selenium库

```
>>> from selenium import webdriver
>>> driver = webdriver.Chrome('/Users/apple/Downloads/chromedriver')
>>> driver.get(request_url)
```

要使用Webdriver，需要去下载<http://npm.taobao.org/mirrors/chromedriver/2.33/> 找到对应的版本，下载完成后再调用添加一下路径就可以调用了。

下载宫崎骏电影海报

对应的XPath为

```python
图片XPath：//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src
名称XPath://div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']
```

notes：XPath获得的图片url格式可能与json格式下不一致，在获得页面内容时，用driver驱动则需要etree.HTML(driver.page_source)否则获得的页面XPath为空

整个下载过程如下:

```python
>>> import json
>>> import requests as req
>>> from lxml import etree
>>> from selenium import webdriver
>>> import os
>>> request_url = 'https://movie.douban.com/subject_search?search_text=宫崎骏&cat=1002'
>>> src_xpath = '//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src'
SyntaxError: invalid syntax
>>> src_xpath = "//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src"
>>> title_xpath = "//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']"
>>> print (src_xpath)
//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src
>>> driver = webdriver.Chrome('/Users/apple/Downloads/chromedriver')
>>> driver.get(request_url)
>>> html = etree.HTML(driver.page_source)
>>> srcs = html.xpath(src_xpath)
>>> print (srcs)
['https://img3.doubanio.com/view/celebrity/s_ratio_celebrity/public/p616.webp', 'https://img3.doubanio.com/f/movie/03d3c900d2a79a15dc1295154d5293a2d5ebd792/pics/movie/tv_default_large.png', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p1606727862.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2540924496.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2174346180.webp', 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p1446261379.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p1613191025.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p456676352.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p454118602.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p456692072.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p1917567652.webp', 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p1998028598.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p617533616.webp', 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2398213627.webp', 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p1637195728.webp', 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p1335271624.webp']
>>> titles = html.xpath(src_xpath）
>>> picpath = '/Users/apple/Downloads/宫崎骏电影海报'
>>> if not os.path.isdir(picpath):
	os.mkdir(picpath)
>>> def download(src, id):
	dic = picpath + '/' + str(id) + '.webp'
	try:
		pic = req.get(src, timeout = 30)
		fp = open(dic, 'wb')
		fp.write(pic.content)
		fp.close()
	except req.exceptions.ConnectionError:
		print ('图片无法下载')
>>> for i in range(0, 150, 15):
	url = request_url + '&start=' + str(i)
	driver.get(url)
	html = etree.HTML(driver.page_source)
	srcs = html.xpath(src_xpath)
	titles = html.xpath(title_xpath)
	for src,title in zip(srcs, titles):
		download(src, title.text)
>>> 

```

