[TOC]

#**SVM**

SVM：Support Vector Machine，支持向量机。在机器学习中是有监督学习模型，可以帮助模式识别、分类以及回归分析。

##**有监督学习&无监督学习**

告诉机器，给它一些数据，这部分数据一些是数据集合A，一部分是属于集合B，然后让机器去把数据往集合A和集合B里去划分，这是有监督学习；同样的数据给机器，只是告诉它去做划分和归类，这是无监督学习，类似于孩子的放养。  

##**超平面**

物体有线性规律摆放，可以用一条直线划分，若物体摆放完全无规律，则需要用曲线来划分。在二维平面上很难让不规律的物体线性划分，若将物体放在更高维度的三维平面划分，则会出现一个水平切面恰好把物体按线性规律划分，该平面就是超平面。

##**SVM工作原理**

SVM就是帮助找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离(分类间隔)最大化。

**分类间隔**：

我们的分类环境不是在二维平面中的，而是在多维空间中，这样直线 C 就变成了决策面 C。在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置。极限的位置是指，如果越过了这个位置，就会产生分类错误。两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。**极限位置到最优决策面  之间的距离，就是分类间隔**。 (英文名叫margin) 

如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解



##**有关超平面距离公式**

**点到超平面的距离公式**：

 					**$g(x) = w^Tx + b, w,x\in R^n​$**

w、x 是 n 维空间里的向量，其中 x 是函数变量；w 是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。

**支持向量**：距离分类超平面最近的样本点。

如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。

**样本集到超平面的距离公式**:

​						$d_i = \dfrac{|wx_i + b|}{||w||}$

其中||w||为超平面的范数，di 的公式可以用解析几何知识进行推导



## **硬间隔、软间隔、核函数**

**硬间隔**：表示得到的最大分类间隔即超平面 能完美的划分数据，不存在划分错误的情况，即零误差
**软间隔**：表示得到的最大分类间隔，没有达到完美的程度，对数据划分存在一定的误差
**核函数**：在数据分布无法用线性函数来表示的时候，需要对数据进行划分的标准变成来非线性的，这个时候就需要用到一种函数名叫核函数，核函数要做的工作是将原来的映射关系在更高维度的空间重新映射，使得新的映射关系变得线性可分。

常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid核，或者是这些核函数的组合。



