[TOC]

#**SVM**

SVM：Support Vector Machine，支持向量机。在机器学习中是有监督学习模型，可以帮助模式识别、分类以及回归分析。

##**有监督学习&无监督学习**

告诉机器，给它一些数据，这部分数据一些是数据集合A，一部分是属于集合B，然后让机器去把数据往集合A和集合B里去划分，这是有监督学习；同样的数据给机器，只是告诉它去做划分和归类，这是无监督学习，类似于孩子的放养。  

##**超平面**

物体有线性规律摆放，可以用一条直线划分，若物体摆放完全无规律，则需要用曲线来划分。在二维平面上很难让不规律的物体线性划分，若将物体放在更高维度的三维平面划分，则会出现一个水平切面恰好把物体按线性规律划分，该平面就是超平面。

##**SVM工作原理**

SVM就是帮助找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离(分类间隔)最大化。

**分类间隔**：

我们的分类环境不是在二维平面中的，而是在多维空间中，这样直线 C 就变成了决策面 C。在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置。极限的位置是指，如果越过了这个位置，就会产生分类错误。两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。**极限位置到最优决策面  之间的距离，就是分类间隔**。 (英文名叫margin) 

如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解

**点到超平面的距离公式**：


 
硬间隔：表示得到的分类间隔即超平面 能完美的划分数据，不存在划分错误的情况，即零误差
软间隔：表示得到的分类间隔，没有达到完美的程度，对数据划分存在一定的误差
核函数：在数据分布无法用线性函数来表示的时候，需要对数据进行划分的标准变成来非线性的，这个时候就需要用到一种函数名叫核函数，核函数要做的工作是将原来的映射关系在更高维度的空间重新映射，使得新的映射关系变得线性可分。